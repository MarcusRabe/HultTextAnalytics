library(methods)
data("data_corpus_inaugural", package = "quanteda")
inaug_dfm = quanteda::dfm(data_corpus_inaugural, verbose = FALSE)
inaug_td = tidy(inaug_dfm)
inaug_tf_idf = inaug_td %>%
bind_tf_idf(term,
document,
count) %>%
arrange(desc(tf_idf))
year_term_counts = inaug_td %>%
extract(document,
"year",
"(\\d+)",
convert = TRUE) %>%
complete(year,
term,
fill = list(count = 0)) %>%
group_by(year) %>%
mutate(year_total = sum(count))
year_term_counts %>%
filter(term %in% c("god", "america", "foreign", "union", "constitution", "freedom")) %>%
ggplot(aes(year,
count / year_total)) +
geom_point() +
geom_smooth() +
facet_wrap(~ term,
scales = "free_y") +
scale_y_continuous(labels = scales::percent_format()) +
ylab("% frequency of word in inaugural address")
data("acq")
acq_td = tidy(acq)
##
data("acq")
acq_td = tidy(acq)
acq_tokens = acq_td %>%
select(-places) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words, by = "word")
install.packages("tm.plugin.webmining")
library(tm.plugin.webmining)
library(purrr)
company = c("Microsoft", "Apple", "Google", "Amazon", "Facebook", "Twitter", "IBM", "Yahoo", "Netflix")
symbol = c("MSFT", "AAPL", "GOOG", "AMZN", "FB", "TWTR", "IBM", "YHOO", "NFLX")
download_articles = function(symbol){
WebCorpus(GoogleFinanceSource(paste0("NASDAQ:", symbol)))
}
stock_articles = data_frame(company = company,
symbol = symbol) %>%
mutate(corpus = map(symbol, download_articles))
library(tm.plugin.webmining)
--request
library(tm.plugin.webmining) --request
download_articles = function(symbol){
WebCorpus(GoogleFinanceSource(paste0("NASDAQ:", symbol)))
}
stock_articles = data_frame(company = company,
symbol = symbol) %>%
mutate(corpus = map(symbol, download_articles))
library(tm.plugin.webmining)
library(purrr)
Sys.setenv(JAVA_HOME=/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home/bin/java)
Sys.setenv(JAVA_HOME=../Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home/bin/java)
library(tm.plugin.webmining)
install.packages("rJava")
library(rJava)
library(rJava)
if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }
# Next, we download packages that H2O depends on.
pkgs <- c("RCurl","jsonlite")
for (pkg in pkgs) {
if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
}
# Now we download, install and initialize the H2O package for R.
install.packages("h2o", type="source", repos="http://h2o-release.s3.amazonaws.com/h2o/rel-xu/3/R")
# Finally, let's load H2O and start up an H2O cluster
library(h2o)
h2o.init()
install.packages("statmod")
pkgs <- c("methods","statmod","stats","graphics","RCurl","jsonlite","tools","utils")for (pkg in pkgs) {if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }}
pkgs <- c("methods","statmod","stats","graphics","RCurl","jsonlite","tools","utils")
for (pkg in pkgs) {if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }}
airlinesTrainData <- h2o.importFile("https://s3.amazonaws.com/h2o-airlines-unpacked/allyears2k.csv")
gbmModel <- h2o.gbm(x=c("Month", "DayOfWeek", "Distance"), y="IsArrDelayed", training_frame = airlinesTrainData)
gbmModel
h2o.varimp(gbmModel)
xgBoostModel <- h2o.xgboost(x=c("Month", "DayOfWeek", "Distance"), y="IsArrDelayed", training_frame = airlinesTrainData)
xgBoostModel
h2o.predict(gbmModel, airlinesTrainData)
summary(mnist_train)
mnist_train
mnist_test = h2o.importFile("https://s3.amazonaws.com/h2o-public-test-data/smalldata/flow_examples/mnist/test.csv.gz")
mnist_train = h2o.importFile("https://s3.amazonaws.com/h2o-public-test-data/smalldata/flow_examples/mnist/train.csv.gz")
mnist_train
summary(mnist_train)
View(mnist_train)
h2o.ls()
h2o.colnames(mnist_train)
ggplot(frequency, aes(x=proportion, y=`Jane Austen`,
color = abs(`Jane Austen`- proportion)))+
geom_abline(color="grey40", lty=2)+
geom_jitter(alpha=.1, size=2.5, width=0.3, height=0.3)+
geom_text(aes(label=word), check_overlap = TRUE, vjust=1.5) +
scale_x_log10(labels = percent_format())+
scale_y_log10(labels= percent_format())+
scale_color_gradient(limits = c(0,0.001), low = "darkslategray4", high = "gray75")+
facet_wrap(~author, ncol=2)+
theme(legend.position = "none")+
labs(y= "Jane Austen", x=NULL)
fb = read.csv("/Users/marcus/Desktop/TA/csv/facebook.csv")
View(fb)
View(fb)
tw = read.csv("/Users/marcus/Desktop/TA/csv/twitter.csv")
ig = read.csv("/Users/marcus/Desktop/TA/csv/instagram.csv")
tidy_fb <- fb$text %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
library(tidytext)
tidy_fb <- fb$text %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
View(fb)
tidy_fb <- toString( fb$text) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
tidy_fb <- fb %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
fb = as.data.frame(fb)
fb = as.data.frame(toString(fb$text))
tidy_fb <- fb %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
fb = as.data.frame(text = toString(fb$text))
fb = as.data.frame(text = fb$text)
fb = read.csv("/Users/marcus/Desktop/TA/csv/facebook.csv")
tidy_fb <- as.Character(fb$text) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
tidy_fb <- as.character(fb$text) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
library(textreadr)
fb = read_document("/Users/marcus/Desktop/TA/csv/facebook.csv")
tw = read_document("/Users/marcus/Desktop/TA/csv/twitter.csv")
ig = read_document("/Users/marcus/Desktop/TA/csv/instagram.csv")
tidy_fb <- fb %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
library(tidytext)
fb = read_document("/Users/marcus/Desktop/TA/csv/facebook.csv")
tw = read_document("/Users/marcus/Desktop/TA/csv/twitter.csv")
ig = read_document("/Users/marcus/Desktop/TA/csv/instagram.csv")
tidy_fb <- fb %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
ig = read_document("/Users/marcus/Desktop/TA/csv/instagram.csv")
library(tibble)
fb = tibble(read_document("/Users/marcus/Desktop/TA/csv/facebook.csv"))
View(fb)
tidy_fb <- fb %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
tidy_fb <- fb %>%
unnest_tokens(word, text)
View(fb)
fb = tibble(text = read_document("/Users/marcus/Desktop/TA/csv/facebook.csv"))
View(fb)
tidy_fb <- fb %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
tw = tibble(text = read_document("/Users/marcus/Desktop/TA/csv/twitter.csv"))
ig = tibble(text = read_document("/Users/marcus/Desktop/TA/csv/instagram.csv"))
library(tm)
library(dplyr)
library(tidytext)
#install.packages("topicmodels")
data("AssociatedPress", package = "topicmodels")
AssociatedPress
#99% of the document-word pairs are zero
terms <- Terms(AssociatedPress)
terms
ap_td <- tidy(AssociatedPress)
ap_td
######################################################
#####Converting back from Tidy to DTM ###############
######################################################
ap_td %>%
cast_dtm(document, term, count )
fb = tidy(read.csv("/Users/marcus/Desktop/TA/csv/facebook.csv"))
View(fb)
tidy_fb <- fb %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
View(tidy_fb)
library(deplyr)
library(tidytext)
fb = read.csv("/Users/marcus/Desktop/TA/csv/facebook.csv")
View(fb)
fb_t = fb$text
fb_t = tidy(fb$text)
View(fb)
fb = as.character(fb)
fb = read.csv("/Users/marcus/Desktop/TA/csv/facebook.csv")
fb_t = tidy(fb$text)
fb_t = tidy(as.character(fb$text))
View(fb_t)
fb_t
View(fb_t)
View(fb)
tidy_fb <- fb_t %>%
unnest_tokens(word, text) %>%
anti_join(stop_words)
fb = read.csv("/Users/marcus/Desktop/TA/csv/facebook.csv")
fb_t = tidy(as.character(fb$text))
tw = read.csv("/Users/marcus/Desktop/TA/csv/twitter.csv")
tw_t = tidy(as.character(tw$text))
ig = read.csv("/Users/marcus/Desktop/TA/csv/instagram.csv")
ig_t = tidy(as.character(ig$text))
ig = read.csv("/Users/marcus/Desktop/TA/csv/instagram.csv")
tidy_fb <- fb_t %>%
unnest_tokens(word, text)
fb_t
tidy_fb <- fb_t %>%
unnest_tokens(word, x)
tidy_fb <- fb_t %>%
unnest_tokens(word, x) %>%
anti_join(stop_words)
library(tidyr)
tidy_fb <- fb_t %>%
unnest_tokens(word, x) %>%
anti_join(stop_words)
library(stringr)
tidy_fb <- fb_t %>%
unnest_tokens(word, x) %>%
anti_join(stop_words)
library(deplyr)
library(tidytext)
library(tidyr)
library(stringr)
library(dplyr)
tidy_fb <- fb_t %>%
unnest_tokens(word, x) %>%
anti_join(stop_words)
data("stop_words")
tidy_fb <- fb_t %>%
unnest_tokens(word, x) %>%
anti_join(stop_words)
stop_words
tidy_fb <- fb_t %>%
unnest_tokens(word, x) %>%
anti_join(stop_words) %>%
count(word, sort = TRUE) %>%
filter(n > 1000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
library(ggplot2)
tidy_fb <- fb_t %>%
unnest_tokens(word, x) %>%
anti_join(stop_words) %>%
count(word, sort = TRUE) %>%
filter(n > 1000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
tidy_fb <- fb_t %>%
unnest_tokens(word, x) %>%
anti_join(stop_words)
tidy_fb %>%
count(word, sort = TRUE) %>%
filter(n > 1000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
View(tidy_fb)
View(tidy_fb)
tidy_fb %>%
count(word, sort = TRUE) %>%
filter(n > 1000)
tidy_fb %>%
count(word, sort = TRUE)
tidy_fb %>%
count(word, sort = TRUE) %>%
filter(n > 20) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
tidy_tw <- tw_t %>%
unnest_tokens(word, x) %>%
anti_join(stop_words)
View(tidy_tw)
tidy_tw %>%
count(word, sort = TRUE)
tidy_tw <- tw_t %>%
unnest_tokens(word, x) %>%
anti_join(stop_words)
tidy_tw %>%
count(word, sort = TRUE) %>%
filter(n > 10) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
shiny::runApp('NikeAdidaS')
runApp('NikeAdidaS')
runApp('NikeAdidaS')
runApp('NikeAdidaS')
runApp('NikeAdidaS')
runApp('NikeAdidaS')
runApp('NikeAdidaS')
runApp('NikeAdidaS')
runApp('NikeAdidaS')
h2o.varimp_plot(baseline_xg)
h2o.shutdown()
library(h2o)
h2o.init()
answers = h2o.importFile("/Users/marcus/Desktop/TA/csv/Data2.csv", header = TRUE, col.names = c("LookFor","FreeT","Age","Gender","AvsN"))
num_train = 45 # records in training dataset
# split into training and test dataset
train = answers[1:num_train,]
test = answers[(num_train+1):(h2o.nrow(answers)-1),]
# columns for baseline ML
predictors = c("Age", "Gender")
response = "AvsN"
# create a XGBoost model (optimized gradient boosting model - supervised learning)
baseline_xg = h2o.xgboost(x = predictors,
y = response,
training_frame = train,
validation_frame = test)
h2o.confusionMatrix(baseline_xg)
h2o.varimp_plot(baseline_xg)
# create a Gradient Boosting model (supervised learning)
baseline_gb = h2o.gbm(x = predictors,
y = response,
training_frame = train,
validation_frame = test,
stopping_metric = "AUC",
stopping_tolerance = 0.001,
score_tree_interval = 10,
stopping_rounds = 5)
h2o.confusionMatrix(baseline_gb)
h2o.varimp_plot(baseline_gb)
# tokenize word columns
data("stop_words")
# tokenized_LF = Q1
tokenized_LF = h2o.tokenize(answers$LookFor, " ") %>%
h2o.tolower()
tokenized_LF = tokenized_LF[!(tokenized_LF %in% stop_words$word),]
# tokenized_FT - Q2
tokenized_FT = h2o.tokenize(answers$FreeT, " ") %>%
h2o.tolower()
tokenized_FT = tokenized_FT[!(tokenized_FT %in% stop_words$word),]
tokenized = h2o.rbind(tokenized_LF,tokenized_FT)
# train a word vector on total tokanized words
w2v = h2o.word2vec(training_frame = tokenized)
# test word vektor
# h2o.findSynonyms(w2v, "style", 5)
# create numeric answer vectors from wordvector
LF_vecs = h2o.transform(word2vec = w2v,
words = tokenized_LF,
aggregate_method = "AVERAGE")
FT_vecs = h2o.transform(word2vec = w2v,
words = tokenized_FT,
aggregate_method = "AVERAGE")
# add wordvector answer columns to original data
answers_ext = h2o.cbind(answers, LF_vecs, FT_vecs)
# create new test and training datasets on extended data
train_ext = answers_ext[1:num_train,]
test_ext = answers_ext[(num_train+1):(h2o.nrow(answers_ext)-1),]
# now train a XGBoost on all columns
extended_xg = h2o.xgboost(y = response,
training_frame = train_ext,
validation_frame = test_ext)
h2o.confusionMatrix(extended_xg)
h2o.varimp_plot(extended_xg)
# GBM on all columns
extended_gb = h2o.gbm(y = response,
training_frame = train_ext,
validation_frame = test_ext,
stopping_metric = "AUC",
stopping_tolerance = 0.001,
score_tree_interval = 10,
stopping_rounds = 5)
h2o.confusionMatrix(extended_gb)
h2o.varimp_plot(extended_gb)
### prepare worddata for Q2
uw = h2o.asfactor(tokenized_FT) %>%
h2o.unique() %>%
h2o.ascharacter()
we = h2o.transform(w2v,
uw,
aggregate_method = "NONE")
we = h2o.cbind(uw, we)
### prepare worddata for Q1
uw_LF = h2o.asfactor(tokenized_LF) %>%
h2o.unique() %>%
h2o.ascharacter()
we_LF = h2o.transform(w2v,
uw_LF,
aggregate_method = "NONE")
we_LF = h2o.cbind(uw_LF,
we_LF)
### look for highest predictors for Q2 GB (C33 is from VarImpPlot)
h2o.hist( we['C33'])
# use the values from the histogram for the threshold here
lowC33 = we[we['C33'] < -0.002,]
lowC33Words = head(lowC33['C1'],3) #top words for Q1 and Nike
highC33 = we[we['C33'] > 0.002,]
highC33Words = head(highC33['C1'],3)  #top words for Q1 and Addidas
### look for highest predictors for Q1 GB (C33 is from VarImpPlot)
h2o.hist( we_LF['C33'])
# use the values from the histogram for the threshold here
lowC33_LF = we_LF[we_LF['C33'] < -0.002,]
lowC33Words_LF = head(lowC33_LF['C1'],3) #top words for Q1 and Nike
highC33_LF = we_LF[we_LF['C33'] > 0.002,]
highC33Words_LF = head(highC33_LF['C1'],3)  #top words for Q1 and Addidas
#########
#XGBoost
h2o.varimp_plot(extended_xg)
### look for highest predictors for Q2 XG (C68 is from VarImpPlot)
h2o.hist( we['C68'])
# use the values from the histogram for the threshold here
lowC68 = we[we['C68'] < -0.003,]
lowC68Words = head(lowC68['C1'],3) #top words for Q1 and Nike
highC68 = we[we['C68'] > 0.003,]
highC68Words = head(highC68['C1'],3)  #top words for Q1 and Addidas
### look for highest predictors for Q1 GB (C68 is from VarImpPlot)
h2o.hist( we_LF['C68'])
# use the values from the histogram for the threshold here
lowC68_LF = we_LF[we_LF['C68'] < -0.002,]
lowC68Words_LF = head(lowC68_LF['C1'],3) #top words for Q1 and Nike
highC68_LF = we_LF[we_LF['C68'] > 0.003,]
highC68Words_LF = head(highC68_LF['C1'],3)  #top words for Q1 and Addidas
############
# Get the most important words from the classifications
ImportantWords = data.frame(Q1_XG_Ad = highC68Words_LF,
Q1_XG_Nike = lowC68Words_LF,
Q2_XG_Ad = highC68Words,
Q2_XG_Nike = lowC68Words,
Q1_GB_Ad = highC33Words_LF,
Q1_GB_Nike = lowC33Words_LF,
Q2_GB_Ad = highC33Words,
Q2_GB_Nike = lowC33Words)
colnames(ImportantWords) = c("Q1_XG_Ad",
"Q1_XG_Nike",
"Q2_XG_Ad",
"Q2_XG_Nike",
"Q1_GB_Ad",
"Q1_GB_Nike",
"Q2_GB_Ad",
"Q2_GB_Nike")
############ predict ust based on age
Nike_LH_BA = data.frame()
for (age in 1:60) {
old_answer = as.h2o(data.frame("bla",
"bla",
age,
"M",
"N"))
colnames(old_answer) = c("LookFor",
"FreeT",
"Age",
"Gender",
"AvsN")
p = h2o.predict(baseline_xg,
old_answer)
Nike_LH_BA = rbind(Nike_LH_BA,
data.frame(age = age,
NLH = as.vector(unlist(p$N))[1]))
}
Nike_LH_BA$ALH = 1-Nike_LH_BA$NLH
# plot the likelihood of someone buying Nike by age
Nike_LH_BA %>%
ggplot(aes(age,
NLH))+
geom_line(size = 1.1,
alpha = 0.8,
show.legend = FALSE)
ImportantWords
runApp('Desktop/TA')
ImportantWords
runApp('Desktop/TA')
runApp('Desktop/TA')
answers = h2o.importFile("./Data2.csv", header = TRUE, col.names = c("LookFor","FreeT","Age","Gender","AvsN"))
library(h2o)
h2o.init()
answers = h2o.importFile("./Data2.csv", header = TRUE, col.names = c("LookFor","FreeT","Age","Gender","AvsN"))
answers = h2o.importFile("Data2.csv", header = TRUE, col.names = c("LookFor","FreeT","Age","Gender","AvsN"))
answers = h2o.importFile("/Data2.csv", header = TRUE, col.names = c("LookFor","FreeT","Age","Gender","AvsN"))
getwd()
setwd("..")
getwd()
dirname(rstudioapi::getSourceEditorContext()$path)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
answers = h2o.importFile("Data2.csv", header = TRUE, col.names = c("LookFor","FreeT","Age","Gender","AvsN"))
getwd()
answers = h2o.importFile("./Data2.csv", header = TRUE, col.names = c("LookFor","FreeT","Age","Gender","AvsN"))
getwd()
path = paste(dirname(rstudioapi::getSourceEditorContext()$path), "/Data2.csv")
path = paste(dirname(rstudioapi::getSourceEditorContext()$path), "/Data2.csv", sep = "")
answers = h2o.importFile(path, header = TRUE, col.names = c("LookFor","FreeT","Age","Gender","AvsN"))
h2o.shutdown()
